#cloud-config

package_update: true
packages:
  - jq
  - curl
  - azure-cli
  - python3

write_files:
  - path: /etc/rancher/k3s/registries.yaml
    permissions: '0644'
    content: |
      mirrors:
        registry.internal:

{{- if eq .Role "control-plane" }}
  - path: /usr/local/bin/update-peers.sh
    permissions: '0755'
    content: |
      #!/bin/bash

      # Variables
      DNS_ZONE_NAME="cluster.internal"
      RESOURCE_GROUP="{{ .ResourceGroup }}"
      RECORD_NAME="kubernetes"
      FULL_RECORD="${RECORD_NAME}.${DNS_ZONE_NAME}"
      INTERVAL_SECONDS=60
      
      while true; do
        echo "Collecting control-plane node IPs..."
        # Get only Ready control-plane node IPs
        CONTROL_PLANE_IPS=$(kubectl get nodes -l node-role.kubernetes.io/control-plane -o json | jq -r '.items[] | select(.status.conditions[] | select(.type=="Ready" and .status=="True")) | .status.addresses[] | select(.type=="InternalIP") | .address')
        
        # Check if record exists first
        if az network private-dns record-set a show --name "$RECORD_NAME" --zone-name "$DNS_ZONE_NAME" --resource-group "$RESOURCE_GROUP" >/dev/null 2>&1; then
          echo "Record set exists, removing existing records..."
          # Record exists, update it instead of delete and recreate
          EXISTING_RECORD=$(az network private-dns record-set a show --name "$RECORD_NAME" --zone-name "$DNS_ZONE_NAME" --resource-group "$RESOURCE_GROUP")
          # Get existing IPs to remove them
          EXISTING_IPS=$(echo $EXISTING_RECORD | jq -r '.aRecords[] | .ipv4Address')
          
          for ip in $EXISTING_IPS; do
            echo "Removing IP $ip from record set..."
            az network private-dns record-set a remove-record \
              --ipv4-address "$ip" \
              --record-set-name "$RECORD_NAME" \
              --zone-name "$DNS_ZONE_NAME" \
              --resource-group "$RESOURCE_GROUP" || true
          done
        else
          echo "Record set does not exist, creating it..."
          # Create a new A record set
          az network private-dns record-set a create \
            --name "$RECORD_NAME" \
            --zone-name "$DNS_ZONE_NAME" \
            --resource-group "$RESOURCE_GROUP" \
            --ttl 60 || true
        fi
        
        # Add each IP to the A record
        for ip in $CONTROL_PLANE_IPS; do
          echo "Adding IP $ip to record set..."
          az network private-dns record-set a add-record \
            --record-set-name "$RECORD_NAME" \
            --zone-name "$DNS_ZONE_NAME" \
            --resource-group "$RESOURCE_GROUP" \
            --ipv4-address "$ip" || true
        done
        
        echo "DNS record $FULL_RECORD updated with control-plane node IPs"
    
        echo "Sleeping for $INTERVAL_SECONDS seconds..."
        sleep "$INTERVAL_SECONDS"
      done

  - path: /etc/systemd/system/k3s-update-peer.service
    permissions: '0644'
    content: |
      [Unit]
      Description=K3s Node Peer Updater
      After=network-online.target k3s.service
      Wants=network-online.target

      [Service]
      Type=simple
      ExecStart=/usr/local/bin/update-peers.sh
      Restart=always
      RestartSec=10
      StandardOutput=journal
      StandardError=journal

      [Install]
      WantedBy=multi-user.target
{{- end }}

runcmd:
  - iptables -A INPUT -p tcp --dport 6443 -j ACCEPT
  - iptables -A INPUT -p tcp --dport 443 -j ACCEPT
  - iptables -A INPUT -p tcp --dport 10250 -j ACCEPT
  - iptables -A INPUT -p tcp --dport 5001 -j ACCEPT
  - iptables -A INPUT -p udp --dport 5001 -j ACCEPT
  - iptables -A INPUT -p icmp -j ACCEPT
  - iptables -A INPUT -p udp --dport 8472 -j ACCEPT
  - iptables -A INPUT -p tcp --dport 2379 -j ACCEPT
  - iptables -A INPUT -p tcp --dport 2380 -j ACCEPT
  - az login --identity --client-id "{{ .MSIClientID }}"
  {{- if .UsePostgres }}
  - export POSTGRES_PASSWORD=$(az keyvault secret show --vault-name "{{.KeyVaultName}}" --name "postgres-admin-password" --query value -o tsv)
  - export POSTGRES_PASSWORD_ESCAPED=$(python3 -c "import urllib.parse,os; print(urllib.parse.quote(os.environ['POSTGRES_PASSWORD']))")
  {{- end }}
  - |
    {{- if eq .Role "control-plane" }}
    {{- if .UsePostgres }}
    curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION="{{ .K8sVersion }}+k3s1" sh -s - server --token=SECRET --tls-san {{ .ExternalIP }} --embedded-registry --cluster-cidr=16.0.0.0/5 --service-cidr=172.20.0.0/16 --kube-controller-manager-arg=node-cidr-mask-size=22 --kubelet-arg=max-pods=1000 --datastore-endpoint="postgres://azureuser:${POSTGRES_PASSWORD_ESCAPED}@{{.PostgresName}}.{{.PostgresSuffix}}:5432/postgres" --etcd-disable-snapshots --disable=traefik,local-path,core-dns,metrics-server --enable-pprof
    {{- else }}
    curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION="{{ .K8sVersion }}+k3s1" sh -s - server --token=SECRET --tls-san {{ .ExternalIP }} --embedded-registry --cluster-cidr=16.0.0.0/5 --service-cidr=172.20.0.0/16 --kube-controller-manager-arg=node-cidr-mask-size=22 --kubelet-arg=max-pods=1000 --datastore-endpoint="{{ .EtcdEndpoint }}" --etcd-disable-snapshots --disable=traefik,local-storage,coredns,metrics-server --enable-pprof
    {{- end }}

    systemctl daemon-reload
    systemctl enable --now k3s-update-peer.service

    # Upload kubeconfig to Key Vault with external IP
    export KUBECONFIG_PATH="/etc/rancher/k3s/k3s.yaml"
    export KUBECONFIG_EXTERNAL_PATH="/tmp/k3s-external.yaml"
    # Wait for kubeconfig to exist
    for i in {1..30}; do
      if [ -f "$KUBECONFIG_PATH" ]; then
        # Replace 127.0.0.1:6443 with provided external IP
        sed "s#https://127.0.0.1:6443#https://{{.ExternalIP}}:6443#g" "$KUBECONFIG_PATH" > "$KUBECONFIG_EXTERNAL_PATH"
        az keyvault secret set --vault-name "{{.KeyVaultName}}" --name "kubeconfig-admin" --file "$KUBECONFIG_EXTERNAL_PATH"
        break
      fi
      sleep 2
    done

    {{- else }}
    curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION="{{ .K8sVersion }}+k3s1" sh -s - agent --token=SECRET --server https://kubernetes.cluster.internal:6443 --kubelet-arg=max-pods=1000
    
    # Wait for k3s to be ready, then label this node as worker
    while ! kubectl --kubeconfig=/var/lib/rancher/k3s/agent/kubelet.kubeconfig get nodes $(hostname) >/dev/null 2>&1; do
      echo "Waiting for node to be ready..."
      sleep 10
    done
    
    # Label this node as a worker
    kubectl --kubeconfig=/var/lib/rancher/k3s/agent/kubelet.kubeconfig label node $(hostname) node-role.kubernetes.io/worker=worker --overwrite
    sleep 5
    {{- end }}

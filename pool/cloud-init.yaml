#cloud-config

package_update: true
packages:
  - jq
  - curl
  - azure-cli
  - python3

write_files:
  - path: /etc/rancher/k3s/registries.yaml
    permissions: '0644'
    content: |
      mirrors:
        registry.internal:

{{- if eq .Role "control-plane" }}
  - path: /usr/local/bin/update-peers.sh
    permissions: '0755'
    content: |
      #!/bin/bash

      # Variables
      DNS_ZONE_NAME="cluster.internal"
      RESOURCE_GROUP="{{ .ResourceGroup }}"
      RECORD_NAME="kubernetes"
      FULL_RECORD="${RECORD_NAME}.${DNS_ZONE_NAME}"
      INTERVAL_SECONDS=60
      
      while true; do
        echo "Collecting control-plane node IPs..."
        # Get only Ready control-plane node IPs
        CONTROL_PLANE_IPS=$(kubectl get nodes -l node-role.kubernetes.io/control-plane -o json | jq -r '.items[] | select(.status.conditions[] | select(.type=="Ready" and .status=="True")) | .status.addresses[] | select(.type=="InternalIP") | .address')
        
        # Check if record exists first
        if az network private-dns record-set a show --name "$RECORD_NAME" --zone-name "$DNS_ZONE_NAME" --resource-group "$RESOURCE_GROUP" >/dev/null 2>&1; then
          echo "Record set exists, removing existing records..."
          # Record exists, update it instead of delete and recreate
          EXISTING_RECORD=$(az network private-dns record-set a show --name "$RECORD_NAME" --zone-name "$DNS_ZONE_NAME" --resource-group "$RESOURCE_GROUP")
          # Get existing IPs to remove them
          EXISTING_IPS=$(echo $EXISTING_RECORD | jq -r '.aRecords[] | .ipv4Address')
          
          for ip in $EXISTING_IPS; do
            echo "Removing IP $ip from record set..."
            az network private-dns record-set a remove-record \
              --ipv4-address "$ip" \
              --record-set-name "$RECORD_NAME" \
              --zone-name "$DNS_ZONE_NAME" \
              --resource-group "$RESOURCE_GROUP" || true
          done
        else
          echo "Record set does not exist, creating it..."
          # Create a new A record set
          az network private-dns record-set a create \
            --name "$RECORD_NAME" \
            --zone-name "$DNS_ZONE_NAME" \
            --resource-group "$RESOURCE_GROUP" \
            --ttl 60 || true
        fi
        
        # Add each IP to the A record
        for ip in $CONTROL_PLANE_IPS; do
          echo "Adding IP $ip to record set..."
          az network private-dns record-set a add-record \
            --record-set-name "$RECORD_NAME" \
            --zone-name "$DNS_ZONE_NAME" \
            --resource-group "$RESOURCE_GROUP" \
            --ipv4-address "$ip" || true
        done
        
        echo "DNS record $FULL_RECORD updated with control-plane node IPs"
    
        echo "Sleeping for $INTERVAL_SECONDS seconds..."
        sleep "$INTERVAL_SECONDS"
      done

  - path: /etc/systemd/system/k3s-update-peer.service
    permissions: '0644'
    content: |
      [Unit]
      Description=K3s Node Peer Updater
      After=network-online.target k3s.service
      Wants=network-online.target

      [Service]
      Type=simple
      ExecStart=/usr/local/bin/update-peers.sh
      Restart=always
      RestartSec=10
      StandardOutput=journal
      StandardError=journal

      [Install]
      WantedBy=multi-user.target
  - path: /usr/local/bin/watch-and-label-nodes.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      while true; do
        kubectl get nodes -o json | jq -r '.items[] | select((.metadata.labels["node-role.kubernetes.io/worker"] == null) and (.metadata.labels["node-role.kubernetes.io/control-plane"] == null) and ((now - (.metadata.creationTimestamp | fromdateiso8601)) > 10)) | .metadata.name' | while read -r node; do
          kubectl label node "$node" node-role.kubernetes.io/worker=worker --overwrite
        done
        sleep 60
      done
  - path: /etc/systemd/system/k3s-node-labeler.service
    permissions: '0644'
    content: |
      [Unit]
      Description=K3s Node Auto-Labeler
      After=network-online.target k3s.service
      Wants=network-online.target

      [Service]
      Type=simple
      ExecStart=/usr/local/bin/watch-and-label-nodes.sh
      Restart=always
      RestartSec=10
      StandardOutput=journal
      StandardError=journal

      [Install]
      WantedBy=multi-user.target
{{- end }}

runcmd:
  - iptables -A INPUT -p tcp --dport 6443 -j ACCEPT
  - iptables -A INPUT -p tcp --dport 443 -j ACCEPT
  - iptables -A INPUT -p tcp --dport 10250 -j ACCEPT
  - iptables -A INPUT -p tcp --dport 5001 -j ACCEPT
  - iptables -A INPUT -p udp --dport 5001 -j ACCEPT
  - iptables -A INPUT -p icmp -j ACCEPT
  - iptables -A INPUT -p udp --dport 8472 -j ACCEPT
  - az login --identity --client-id "{{ .MSIClientID }}"
  - |
    {{- if eq .Role "control-plane" }}
    curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION="{{ .K8sVersion }}+k3s1" sh -s - server --token=SECRET --tls-san {{ .ExternalIP }} --embedded-registry

    systemctl daemon-reload
    systemctl enable --now k3s-node-labeler.service
    systemctl enable --now k3s-update-peer.service

    # Upload kubeconfig to Key Vault with external IP
    export KUBECONFIG_PATH="/etc/rancher/k3s/k3s.yaml"
    export KUBECONFIG_EXTERNAL_PATH="/tmp/k3s-external.yaml"
    # Wait for kubeconfig to exist
    for i in {1..30}; do
      if [ -f "$KUBECONFIG_PATH" ]; then
        # Replace 127.0.0.1:6443 with provided external IP
        sed "s#https://127.0.0.1:6443#https://{{.ExternalIP}}:6443#g" "$KUBECONFIG_PATH" > "$KUBECONFIG_EXTERNAL_PATH"
        az keyvault secret set --vault-name "{{.KeyVaultName}}" --name "kubeconfig-admin" --file "$KUBECONFIG_EXTERNAL_PATH"
        break
      fi
      sleep 2
    done

    {{- else }}
    curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION="{{ .K8sVersion }}+k3s1" sh -s - agent --token=SECRET --server https://kubernetes.cluster.internal:6443
    sleep 5
    {{- end }}
